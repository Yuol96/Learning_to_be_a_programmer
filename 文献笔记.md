# 2014 Large-scale Video classification with ConvNets

[paper link](https://www-cs.stanford.edu/groups/vision/pdf/karpathy14.pdf) 

### contributions

- approaches for extending CNNs into video clas- sification on Sports487-1M

  - **time infomation fusion**: extend the connectivity of the network in time dimension to learn spatio-temporal features 

  ![](https://img-blog.csdn.net/20170929152950249?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1eGlhbzIxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

- Architecture processing input at 2 spatial resolutions

  - **Low-resolution context stream**: receives the downsam- pled frames at half the original spatial resolution 
  - **High-resolution fovea stream**: receives the center 89 × 89 region at the original resolution 

  ![](https://img-blog.csdn.net/20170929153003580?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1eGlhbzIxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

- improvements made by Transfer learning to other datasets

### some details

- treat every video as a bag of short, fixed-sized clips 

----

# 2014 two-stream ConvNet

![](https://img-blog.csdn.net/20171208151401719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGl1eGlhbzIxNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast) 

[paper link](https://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf) 

UCF-11 87.0%

### contributions

- Combines **spatial** & **temporal** network followed by **class score fusion**
  - **spatial:** take a still frame as input and perform action recognition
    - Scene & object
  - **temporal:** take a 2L-channel optical flow/trajectory stacking corresponding to the still frame as input and perform action recognition in this multi-channel input. 
    - movement of camera & objects
  -  **fusion:**  two outputs of the streams are concated as a feature to train a SVM classifier to fuse them.
- ConNet trained on **multi-frame dense optical flow**
  - Achieve good performance in spite of small training dataset
- **Multi-task training** procedure
  - combine several datasets and increase the amount of training data
  - Benefits performance on different datasets

### some details

- **Optical flow stacking** & **trajectory stacking** 的区别
  - optical flow stacking：在各个frame的同一个坐标不同object上的displacement vector
  - Trjectory stacking: 在各个frame的同一个object不同坐标（因为在运动）上的displacement vector
- **Bi-directional optical flow**
  - Construct an input volume $I_\tau$ by stacking frames between $\tau-L/2$ and $\tau+L/2$ 
- **Mean flow subtraction**
  - is utilized to eliminate displacements caused by camera movement.
  - from each displacement field **d** we subtract its mean vector. 
- **Sample fixed-size input** sub-volume $224\times224\times2L$from$I_\tau\in\mathbb{R}^{w\times h\times2L}$ 
- At test stage, 25 frames (time points) are extracted and their corresponding 2L-channel stackings are sent to the network. In addition, 5 patches and their flips are extracted in space domain.

### pros & cons

- 缺点：cannot localize action in neither spatial nor temporal domain

### Questions

- L通常多大？
- spatial stream只有一个frame？
- 怎么实现different softmax layer for different datasets (multi-task learning) ?
- 最后是怎么 用SVM fuse softmax scores的?

### Blogs references

[READING NOTE: Two-Stream Convolutional Networks for Action Recognition in Videos](https://blog.csdn.net/joshua_1988/article/details/49688309) 

[【论文学习】Two-Stream总结](https://blog.csdn.net/liuxiao214/article/details/78748847) 



-----

# 2015 Very Deep Two-Stream ConvNets 

[paper link](https://arxiv.org/pdf/1507.02159) 

UCF-11 91.4%

### Background

- 模型深度太浅，5层卷积+3层FC
- 行为识别的数据集规模太小
  - UCF-101只有13320个clips

### contributions

- Very deep two-stream
  - Spatial: $224\times224\times3$
  - Temporal: 时域为10的光流场 $224\times224\times10$ 

### some details

- UCF-101: 10000个视频用于train，3300个用于测试
- **预训练**：
  - 空间流在ImageNet上预训练，
  - 时间流中的光流转换为0-255灰度图；在所有通道上，对ImageNet模型滤波器的第一层求平均值，然后将结果复制20次作为时域网络的初始化值，再在ImageNet上预训练。
- **更小的learning_rate**：
  - temporal为0.005，每1万次迭代减少1/10，3万次停止。
  - Spatial为0.001，每4000次迭代减少1/10，1万次停止。
- **more data—data argumentation**：
  - **角落裁剪策略**：由于数据集过小的原因，采用裁剪增加数据集，4个角和1个中心，还有各种尺度的裁剪。从{26,224,192,168}中选择尺度与纵横比进行裁剪。
  - **Horizontal flip**
  - 因此每个frame得到10个输入（四个角一个中心，再水平翻转）
- **high dropout rate**
- 多GPU训练
- 为了空域网络和时域网络的融合，对它们的预测分数采用加权平均，时域权重为2，空域权重为1.

### pros & cons

- 

### Questions

- 

### Blogs references

[【论文学习】Two-Stream总结](https://blog.csdn.net/liuxiao214/article/details/78748847) 

[《Towards Good Practices for Very Deep Two-Stream ConvNets》阅读笔记](https://blog.csdn.net/lk274857347/article/details/77645586) 

----

# 2015 3D ConvNets & C3D

[paper link](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf) 

### contributions

- deep 3D ConvNets modeling appearance and motion simultaneously
- find $3\times3\times3$ 卷积核是实验效果最好的
- C3D feature
  - useful for various tasks without requiring to finetune the model for each task 

### some details

- 

### pros & cons

- 

### Questions

- 

### Blogs references

----

# 2016 TSN

[paper link](https://arxiv.org/pdf/1608.00859) 

UCF101 94.2%

HMDB51 69.4%

![](https://img-blog.csdn.net/20170713162331190?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2VpeGluXzM3OTcwNjk0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 

### Background

- 做不到long-range视频的理解
  - 主要是short-term motions
  - 有dense temporal sampling with a pre-defined sampling interval ，但对long video计算代价太大
- 视频资源规模太小

### contributions

- Sparse temporal sampling strategy提取short snippets
  - Samples distribute uniformly aong the temporal dimension

### some details

- Adopt Very Deep ConvNet architectures
- 4 types of input modalities to two-stream ConvNets
  - Single RGB image
  - stacked RGB difference
  - Stacked optical flow field
  - stacked warped optical flow field
- Cross-modality pre-training
- regularization
- Enhanced data augmentation

### pros & cons

- 

### Questions

- 

### Blogs references

[【论文学习】Two-Stream总结](https://blog.csdn.net/liuxiao214/article/details/78748847) 



----

# 20xx 模板

[paper link]()

(Results… on …datasets)

### Background

- 

### contributions

- 

### some details

- 

### pros & cons

- 

### Questions

- 

### Blogs references